{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_bilstm",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sauravsingla/Multi-Class-Sentiment-Classification-/blob/main/twitter_bilstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6X_oFqGUUCQ"
      },
      "source": [
        " from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn78ZVGj3t6e"
      },
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, Embedding, Input, Flatten\n",
        "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import np_utils\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "#########PREPROCESSING PART #################################################\n",
        "df = pd.read_csv(\"/content/drive/My Drive/NLP/text_emotion.csv\")\n",
        "df = df.drop([\"tweet_id\",\"author\"],axis = 1)\n",
        "print(df.head())\n",
        "\n",
        "df['target'] = df['sentiment'].map({'sadness': 0, 'boredom':1,'neutral':2,'worry':3,'surprise':4,'love':5,'fun':6,'hate':7,'happiness':8,'anger':9,'relief':10,'enthusiasm':11,'empty':12})\n",
        "print(df.head())\n",
        "\n",
        "df = df.drop([\"sentiment\"],axis=1)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-w_lJhsct6M"
      },
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"i'm\", \"i am\", text)\n",
        "    text = re.sub(r\"he's\", \"he is\", text)\n",
        "    text = re.sub(r\"it's\", \"it is\", text)\n",
        "    text = re.sub(r\"she's\", \"she is\", text)\n",
        "    text = re.sub(r\"that's\", \"that is\", text)\n",
        "    text = re.sub(r\"what's\", \"what is\", text)\n",
        "    text = re.sub(r\"where's\", \"where is\", text)\n",
        "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
        "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
        "    text = re.sub(r\"\\'re\", \" are\", text)\n",
        "    text = re.sub(r\"\\'d\", \" would\", text)\n",
        "    text = re.sub(r\"won't\", \"will not\", text)\n",
        "    text = re.sub(r\"don't\", \"do not\", text)\n",
        "    text = re.sub(r\"can't\", \"cannot\", text)\n",
        "    text = re.sub(r\"didn't\", \"did not\", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMLwbBEscwwr"
      },
      "source": [
        "cleaned_text = []\n",
        "for text in df['content']:\n",
        "    cleaned_text.append(clean_text(text))\n",
        "df['clean'] = cleaned_text\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xyehs-ehcztz"
      },
      "source": [
        "tw = []\n",
        "for j in df['clean']:\n",
        "  tweets = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", str(j)).split())\n",
        "  tw.append(tweets)\n",
        "df['clean'] = tw\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ik7fjoDSdsad"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stopwords = set(stopwords.words('english'))\n",
        "output = []\n",
        "for sentence in df[\"clean\"]:\n",
        "    temp_list = []\n",
        "    for word in sentence.split():\n",
        "      if len(word) > 2 not in stopwords:\n",
        "        temp_list.append(word)\n",
        "    output.append(' '.join(temp_list))\n",
        "    \n",
        "df[\"texts\"] = output\n",
        "df = df.drop(['content','clean'], axis = 1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzp-aY7LeHGj"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "\n",
        "def lemmatize_text(texts):\n",
        "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(texts)]\n",
        "\n",
        "\n",
        "df['text_lemmatized'] = df.texts.apply(lemmatize_text)\n",
        "\n",
        "sc = [[' '.join(i)] for i in df['text_lemmatized']]\n",
        "lis = []\n",
        "for i in sc:\n",
        "    abc = i[0]\n",
        "    lis.append(abc)\n",
        "\n",
        "df['lem'] = lis\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRAgVLqEd5Y3"
      },
      "source": [
        "df = df.drop(['text_lemmatized','texts'], axis =1)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f6_qJC4bxRf"
      },
      "source": [
        "df.drop(df[df['target'] == 12].index, inplace = True)\n",
        "df.drop(df[df['target'] == 11].index, inplace = True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPwpGMFHZoK6"
      },
      "source": [
        "X = df[\"lem\"]\n",
        "Y = df[\"target\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaa9scl54hfs"
      },
      "source": [
        "# some configuration\n",
        "MAX_SEQUENCE_LENGTH = 48\n",
        "MAX_VOCAB_SIZE = 20000\n",
        "EMBEDDING_DIM = 100\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 100\n",
        "\n",
        "# load in pre-trained word vectors\n",
        "print('Loading word vectors...')\n",
        "word2vec = {}\n",
        "with open(os.path.join('/content/drive/My Drive/NLP/glove.6B.%sd.txt' % EMBEDDING_DIM),encoding='utf-8') as f:\n",
        "  # is just a space-separated text file in the format:\n",
        "  # word vec[0] vec[1] vec[2] ...\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vec = np.asarray(values[1:], dtype='float32')\n",
        "    word2vec[word] = vec\n",
        "print('Found %s word vectors.' % len(word2vec))\n",
        "\n",
        "# convert the sentences (strings) into integers\n",
        "tokenizer = Tokenizer(num_words=MAX_VOCAB_SIZE)\n",
        "tokenizer.fit_on_texts(X)\n",
        "sequences = tokenizer.texts_to_sequences(X)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word2idx))\n",
        "\n",
        "\n",
        "# pad sequences so that we get a N x T matrix\n",
        "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "\n",
        "Xtrain, Xtest, Ytrain, Ytest = train_test_split(data,Y, test_size = 0.3)\n",
        "Yt = np_utils.to_categorical(Ytrain)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1SbdQQ4X9Qb"
      },
      "source": [
        "print('Filling pre-trained embeddings...')\n",
        "num_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\n",
        "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
        "for word, i in word2idx.items():\n",
        "  if i < MAX_VOCAB_SIZE:\n",
        "    embedding_vector = word2vec.get(word)\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "      \n",
        "embedding_layer = Embedding(\n",
        "  num_words,\n",
        "  EMBEDDING_DIM,\n",
        "  weights=[embedding_matrix],\n",
        "  input_length=MAX_SEQUENCE_LENGTH,\n",
        "  trainable=False\n",
        ")\n",
        "\n",
        "print('Building model...')\n",
        "\n",
        "# create an LSTM network with a single LSTM\n",
        "\n",
        "input_ = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "x = embedding_layer(input_)\n",
        "x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
        "x = Dropout(0.2)(x)\n",
        "x = Bidirectional(LSTM(30, return_sequences=True))(x)\n",
        "x = GlobalMaxPool1D()(x)\n",
        "output = Dense(11, activation=\"softmax\")(x)\n",
        "\n",
        "model = Model(input_, output)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['accuracy'],\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hv-L2mJC5Gwx"
      },
      "source": [
        "print('Training model...')\n",
        "r = model.fit(\n",
        "  Xtrain,\n",
        "  Yt,\n",
        "  batch_size=BATCH_SIZE,\n",
        "  epochs=10,\n",
        "  validation_split=0.4\n",
        ")\n",
        "\n",
        "\n",
        "plt.plot(r.history['loss'], label='loss')\n",
        "plt.plot(r.history['val_loss'], label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# accuracies\n",
        "plt.plot(r.history['accuracy'], label='acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val_acc')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFF7VqwqYEYE"
      },
      "source": [
        "p = model.predict(Xtest)\n",
        "print(p)\n",
        "p = np.argmax(p, axis=1)\n",
        "print(p)\n",
        "\n",
        "cnfv=confusion_matrix(Ytest,p)\n",
        "print(cnfv)\n",
        "\n",
        "print(classification_report(Ytest, p))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}